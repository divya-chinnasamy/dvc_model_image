{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "import os\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 0 files [00:00, ? files/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 400 files [00:00, 748.41 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "\n",
    "input_folder = 'data/'\n",
    "splitfolders.ratio(input_folder, output=\"output_folder\", seed=42, ratio=(.7, .2, .1), group_prefix=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your folder containing images\n",
    "train_path=\"output_folder/train\"\n",
    "test_path=\"output_folder/test\"\n",
    "val_path=\"output_folder/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the NumPy array: (280, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the image arrays\n",
    "x_train = []\n",
    "\n",
    "# Iterate over all image files in the folder\n",
    "for filename in os.listdir(train_path):\n",
    "    sub_path=train_path+\"/\"+ filename\n",
    "    for img in os.listdir(sub_path):\n",
    "        image_path=sub_path+\"/\"+img\n",
    "        img_arr=cv2.imread(image_path)\n",
    "        img_arr=cv2.resize(img_arr,(224,224))\n",
    "        x_train.append(img_arr)\n",
    "\n",
    "# Convert the list of arrays into a NumPy array\n",
    "x_train_np = np.array(x_train)\n",
    "\n",
    "# Print the shape of the resulting array (should be something like (num_images, 256, 256, 3))\n",
    "print(\"Shape of the NumPy array:\", x_train_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the NumPy array: (40, 224, 224, 3)\n",
      "Shape of the NumPy array: (80, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "x_test=[]\n",
    "for folder in os.listdir(test_path):\n",
    "    sub_path=test_path+\"/\"+folder\n",
    "    for img in os.listdir(sub_path):\n",
    "        image_path=sub_path+\"/\"+img\n",
    "        img_arr=cv2.imread(image_path)\n",
    "        img_arr=cv2.resize(img_arr,(224,224))\n",
    "        x_test.append(img_arr)\n",
    "\n",
    "x_val=[]\n",
    "for folder in os.listdir(val_path):\n",
    "    sub_path=val_path+\"/\"+folder\n",
    "    for img in os.listdir(sub_path):\n",
    "        image_path=sub_path+\"/\"+img\n",
    "        img_arr=cv2.imread(image_path)\n",
    "        img_arr=cv2.resize(img_arr,(224,224))\n",
    "        x_val.append(img_arr)\n",
    "\n",
    "# Convert the list of arrays into a NumPy array\n",
    "x_test_np = np.array(x_test)\n",
    "x_val_np = np.array(x_val)\n",
    "# Print the shape of the resulting array (should be something like (num_images, 256, 256, 3))\n",
    "print(\"Shape of the NumPy array:\", x_test_np.shape)\n",
    "print(\"Shape of the NumPy array:\", x_val_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "train_x=x_train_np/255.0 \n",
    "test_x=x_test_np/255.0 \n",
    "val_x=x_val_np/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.legacy.preprocessing.image.ImageDataGenerator object at 0x000001E07C750920>\n"
     ]
    }
   ],
   "source": [
    "# ImageDataGenerator for data augmentation and rescaling\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "print(train_datagen)\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 280 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "<keras.src.legacy.preprocessing.image.ImageDataGenerator object at 0x000001E07C750920>\n",
      "<keras.src.legacy.preprocessing.image.DirectoryIterator object at 0x000001E07C6F3320>\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    class_mode='categorical'  \n",
    ")\n",
    "\n",
    "# Load and preprocess the validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    val_path,\n",
    "    target_size=(224, 224),\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "print(train_datagen)\n",
    "print(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Divyaprabha\\.conda\\envs\\ml-pipeline\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Build a model\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(224, 224,3)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150528</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">19,267,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150528\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m19,267,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,267,970</span> (73.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,267,970\u001b[0m (73.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,267,970</span> (73.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,267,970\u001b[0m (73.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # Use 'binary_crossentropy' for binary classification\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Divyaprabha\\.conda\\envs\\ml-pipeline\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  9/140\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 194ms/step - accuracy: 0.5594 - loss: 50.6766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Divyaprabha\\.conda\\envs\\ml-pipeline\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.5640 - loss: 39.4557 - val_accuracy: 0.6750 - val_loss: 1.3918\n",
      "Epoch 2/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5620 - loss: 2.6455 - val_accuracy: 0.4875 - val_loss: 2.6452\n",
      "Epoch 3/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6916 - loss: 1.1833 - val_accuracy: 0.6500 - val_loss: 1.4340\n",
      "Epoch 4/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6945 - loss: 1.2195 - val_accuracy: 0.6125 - val_loss: 1.4815\n",
      "Epoch 5/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8299 - loss: 0.5716 - val_accuracy: 0.7500 - val_loss: 0.8523\n",
      "Epoch 6/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8374 - loss: 0.4846 - val_accuracy: 0.7375 - val_loss: 0.7691\n",
      "Epoch 7/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8928 - loss: 0.2948 - val_accuracy: 0.7875 - val_loss: 0.9246\n",
      "Epoch 8/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8803 - loss: 0.3128 - val_accuracy: 0.7750 - val_loss: 0.6700\n",
      "Epoch 9/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8095 - loss: 0.4027 - val_accuracy: 0.7750 - val_loss: 0.6786\n",
      "Epoch 10/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8751 - loss: 0.2830 - val_accuracy: 0.7875 - val_loss: 0.6830\n",
      "Epoch 11/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9548 - loss: 0.1636 - val_accuracy: 0.7875 - val_loss: 0.6590\n",
      "Epoch 12/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9462 - loss: 0.1450 - val_accuracy: 0.8000 - val_loss: 0.6348\n",
      "Epoch 13/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9646 - loss: 0.1297 - val_accuracy: 0.8000 - val_loss: 0.5972\n",
      "Epoch 14/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9749 - loss: 0.1079 - val_accuracy: 0.7750 - val_loss: 0.7336\n",
      "Epoch 15/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9854 - loss: 0.0965 - val_accuracy: 0.8125 - val_loss: 0.5892\n",
      "Epoch 16/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9783 - loss: 0.0888 - val_accuracy: 0.8125 - val_loss: 0.5711\n",
      "Epoch 17/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9856 - loss: 0.0816 - val_accuracy: 0.8250 - val_loss: 0.5755\n",
      "Epoch 18/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9927 - loss: 0.0684 - val_accuracy: 0.8375 - val_loss: 0.5825\n",
      "Epoch 19/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9894 - loss: 0.0657 - val_accuracy: 0.7875 - val_loss: 0.5996\n",
      "Epoch 20/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0550 - val_accuracy: 0.8000 - val_loss: 0.5848\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, \n",
    "          steps_per_epoch = train_generator.samples // 2,\n",
    "          epochs=20, \n",
    "         validation_data=validation_generator,\n",
    "         validation_steps=validation_generator.samples // 2,\n",
    "         verbose=1)\n",
    "\n",
    "model.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7750 - loss: 0.6374\n",
      "Validation Accuracy: 80.00%\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.0474\n",
      "Train Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "train_loss, train_accuracy = model.evaluate(train_generator)\n",
    "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(40, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Prediction using trained model\n",
    "print(type(test_x))\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "[[9.8973364e-01 1.0266399e-02]\n",
      " [9.9996376e-01 3.6241017e-05]\n",
      " [9.3819886e-01 6.1801068e-02]\n",
      " [9.6005243e-01 3.9947558e-02]\n",
      " [4.4293955e-01 5.5706042e-01]\n",
      " [2.9213564e-02 9.7078639e-01]\n",
      " [9.9984109e-01 1.5883555e-04]\n",
      " [9.9992919e-01 7.0773844e-05]\n",
      " [2.0191494e-01 7.9808503e-01]\n",
      " [9.9948597e-01 5.1399320e-04]\n",
      " [1.2322666e-01 8.7677336e-01]\n",
      " [2.0504861e-01 7.9495132e-01]\n",
      " [9.9999905e-01 9.4997353e-07]\n",
      " [8.6959201e-01 1.3040800e-01]\n",
      " [9.9325037e-01 6.7496160e-03]\n",
      " [2.2203256e-01 7.7796739e-01]\n",
      " [7.8922200e-01 2.1077806e-01]\n",
      " [9.9129069e-01 8.7093255e-03]\n",
      " [9.1623414e-01 8.3765879e-02]\n",
      " [9.5761633e-01 4.2383693e-02]\n",
      " [8.7038565e-01 1.2961431e-01]\n",
      " [8.3968267e-03 9.9160320e-01]\n",
      " [9.7663358e-02 9.0233666e-01]\n",
      " [4.5239949e-03 9.9547607e-01]\n",
      " [9.4329447e-01 5.6705557e-02]\n",
      " [4.8888388e-01 5.1111609e-01]\n",
      " [9.8745036e-01 1.2549611e-02]\n",
      " [6.8026745e-01 3.1973258e-01]\n",
      " [9.1398247e-02 9.0860170e-01]\n",
      " [8.9574300e-02 9.1042572e-01]\n",
      " [3.9545882e-01 6.0454118e-01]\n",
      " [8.2073116e-01 1.7926885e-01]\n",
      " [5.3729463e-01 4.6270537e-01]\n",
      " [6.2454836e-03 9.9375457e-01]\n",
      " [1.1151039e-01 8.8848960e-01]\n",
      " [9.7294623e-01 2.7053824e-02]\n",
      " [4.9108216e-01 5.0891781e-01]\n",
      " [7.8863788e-01 2.1136212e-01]\n",
      " [1.7220527e-01 8.2779473e-01]\n",
      " [3.2801214e-01 6.7198795e-01]]\n",
      "Predicted classes: [0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1\n",
      " 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Batch Prediction\n",
    "predictions = model.predict(test_x)\n",
    "\n",
    "print(predictions)\n",
    "# If you want to get the predicted class for each image\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print predicted classes\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "[[6.035186e-13 1.000000e+00]]\n",
      "Predicted class: [1]\n"
     ]
    }
   ],
   "source": [
    "#Single image Prediction\n",
    "pred_path = '20.jpg'\n",
    "img = image.load_img(pred_path, target_size=(224,224))\n",
    "\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = img_array / 255.0 \n",
    "\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "print(predictions) \n",
    "\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "model_signature = infer_signature(test_x, model.predict(test_x))\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "input_data_path = 'v_data'\n",
    "exp_timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "exp_name = \"custom_ds\" + exp_timestamp\n",
    "print(exp_name)\n",
    "\n",
    "run_timestamp = datetime.now().strftime(\"%Y%m%d--%H%M%S\")\n",
    "run_name = \"custom_ds_run\"+ run_timestamp\n",
    "print(run_name)\n",
    "\n",
    "#mlflow.tensorflow.autolog(log_every_epoch=2)\n",
    "\n",
    "mlflow.set_experiment(exp_name)\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "        #mlflow.log_artifact(input_data_path, artifact_path=\"input_data\")\n",
    "        mlflow.log_param(\"batch_size\", 2)\n",
    "        #mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"epochs\", 20)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss)\n",
    "        mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "        \n",
    "        mlflow.tensorflow.log_model(model, \"custom_ds\", signature=model_signature,registered_model_name='custom_ds_model')\n",
    "mlflow.end_run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
